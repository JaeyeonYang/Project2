import PyPDF2
import google.generativeai as genai  # Gemini API ì¶”ê°€
import os
import json
import re
from typing import Dict, List, Optional
from dotenv import load_dotenv
import asyncio
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from collections import Counter

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class KeywordExtractor:
    def __init__(self):
        # Gemini API í‚¤ ì„¤ì • (ìš°ì„ ìˆœìœ„)
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-pro')
        
        # OpenAI API í‚¤ ì„¤ì • (í´ë°±ìš©)
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # ì—°êµ¬ ë¶„ì•¼ í‚¤ì›Œë“œ ì‚¬ì „ (í´ë°±ìš©)
        self.research_keywords = {
            "ai_ml": ["artificial intelligence", "machine learning", "deep learning", "neural network", 
                     "AI", "ML", "DL", "CNN", "RNN", "LSTM", "GAN", "transformer", "bert", "gpt"],
            "computer_vision": ["computer vision", "image processing", "opencv", "CNN", "object detection",
                               "segmentation", "classification", "recognition", "medical imaging"],
            "nlp": ["natural language processing", "NLP", "text mining", "sentiment analysis", 
                   "language model", "tokenization", "embedding", "BERT", "GPT"],
            "robotics": ["robotics", "autonomous", "control system", "sensor", "navigation", 
                        "manipulation", "ROS", "motion planning"],
            "data_science": ["data science", "data analysis", "statistics", "visualization", 
                            "pandas", "numpy", "matplotlib", "seaborn", "jupyter"],
            "programming": ["python", "java", "c++", "javascript", "tensorflow", "pytorch", 
                           "scikit-learn", "keras", "react", "node.js", "git", "docker"],
            "research_methods": ["research", "experiment", "analysis", "methodology", "publication",
                                "conference", "journal", "paper", "thesis", "dissertation"] 
        }
    
    def is_gemini_configured(self) -> bool:
        """Gemini API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return bool(self.gemini_api_key)
    
    def is_openai_configured(self) -> bool:
        """OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return bool(self.openai_api_key)
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
                
                return text.strip()
        
        except Exception as e:
            raise Exception(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        text = re.sub(r'[^\w\s\.\-\+\#]', ' ', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        return text
    
    async def extract_with_gemini(self, text: str) -> Dict:
        """Gemini APIë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            prompt = f"""
ë‹¤ìŒì€ ì—°êµ¬ìì˜ CV í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ CVì—ì„œ ì—°êµ¬ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì„œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ìš©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

CV í…ìŠ¤íŠ¸:
{text[:6000]}  # GeminiëŠ” ë” ë§ì€ í† í° ì²˜ë¦¬ ê°€ëŠ¥

ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "research_fields": ["AI", "Machine Learning", "Computer Vision"],
    "technologies": ["Python", "TensorFlow", "PyTorch"],
    "methods": ["Deep Learning", "CNN", "Transfer Learning"], 
    "applications": ["Medical Imaging", "Natural Language Processing"],
    "confidence": "high"
}}

ì£¼ì˜ì‚¬í•­:
- ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ í¬í•¨
- ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ˆ: "computer", "software")
- ì‹ ë¢°ë„ëŠ” high/medium/low ì¤‘ í•˜ë‚˜
- ì •í™•í•œ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µ
"""

            if not self.gemini_api_key:
                raise Exception("Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            try:
                # Gemini API í˜¸ì¶œ
                response = self.gemini_model.generate_content(prompt)
                content = response.text.strip()
                
            except Exception as e:
                print(f"Gemini API ì˜¤ë¥˜: {str(e)}")
                raise Exception(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # ```jsonìœ¼ë¡œ ê°ì‹¸ì§„ ê²½ìš° ì²˜ë¦¬
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0].strip()
                elif '```' in content:
                    content = content.split('```')[1].strip()
                
                result = json.loads(content)
                return {
                    "method": "gemini",
                    "keywords": self._flatten_keywords(result),
                    "categories": result,
                    "confidence": result.get("confidence", "medium")
                }
            except json.JSONDecodeError as e:
                print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
                keywords = self._extract_keywords_from_text(content)
                return {
                    "method": "gemini_text",
                    "keywords": keywords,
                    "categories": {"extracted": keywords},
                    "confidence": "low"
                }
        
        except Exception as e:
            print(f"Gemini í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"Gemini í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    async def extract_with_openai(self, text: str) -> Dict:
        """OpenAI APIë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í´ë°±ìš©)"""
        try:
            import openai  # í•„ìš”ì‹œì—ë§Œ import
            
            prompt = f"""
ë‹¤ìŒì€ ì—°êµ¬ìì˜ CV í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ CVì—ì„œ ì—°êµ¬ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì„œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ìš©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

CV í…ìŠ¤íŠ¸:
{text[:4000]}  # í† í° ì œí•œì„ ìœ„í•´ 4000ìë¡œ ì œí•œ

ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "research_fields": ["AI", "Machine Learning", "Computer Vision"],
    "technologies": ["Python", "TensorFlow", "PyTorch"],
    "methods": ["Deep Learning", "CNN", "Transfer Learning"], 
    "applications": ["Medical Imaging", "Natural Language Processing"],
    "confidence": "high"
}}

ì£¼ì˜ì‚¬í•­:
- ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ í¬í•¨
- ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ˆ: "computer", "software")
- ì‹ ë¢°ë„ëŠ” high/medium/low ì¤‘ í•˜ë‚˜
- ì •í™•í•œ JSON í˜•íƒœë¡œë§Œ ì‘ë‹µ
"""

            if not self.openai_api_key:
                raise Exception("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            openai.api_key = self.openai_api_key
            
            try:
                response = await openai.ChatCompletion.acreate(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì—°êµ¬ìì˜ CVì—ì„œ ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=800
                )
            except Exception as e:
                print(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
                raise Exception(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(content)
                return {
                    "method": "openai",
                    "keywords": self._flatten_keywords(result),
                    "categories": result,
                    "confidence": result.get("confidence", "medium")
                }
            except json.JSONDecodeError as e:
                print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                keywords = self._extract_keywords_from_text(content)
                return {
                    "method": "openai_text",
                    "keywords": keywords,
                    "categories": {"extracted": keywords},
                    "confidence": "low"
                }
        
        except Exception as e:
            print(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise Exception(f"OpenAI í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def extract_with_fallback(self, text: str) -> Dict:
        """í´ë°± ë°©ì‹: ìˆ˜ë™ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("ğŸ“‹ í´ë°± ëª¨ë“œ: ìˆ˜ë™ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰")
        
        text_lower = text.lower()
        found_keywords = {
            "research_fields": [],
            "technologies": [],
            "methods": [],
            "applications": []
        }
        
        # ì‚¬ì „ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
        for category, keywords in self.research_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    if category in ["ai_ml", "computer_vision", "nlp", "robotics"]:
                        found_keywords["research_fields"].append(keyword)
                    elif category == "programming":
                        found_keywords["technologies"].append(keyword)
                    elif category == "research_methods":
                        found_keywords["methods"].append(keyword)
                    else:
                        found_keywords["applications"].append(keyword)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        for category in found_keywords:
            found_keywords[category] = list(set(found_keywords[category]))[:8]
        
        # TF-IDFë¡œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ê°€
        try:
            additional_keywords = self._extract_with_tfidf(text)
            found_keywords["applications"].extend(additional_keywords[:5])
            found_keywords["applications"] = list(set(found_keywords["applications"]))[:8]
        except:
            pass
        
        all_keywords = self._flatten_keywords(found_keywords)
        
        return {
            "method": "fallback",
            "keywords": all_keywords,
            "categories": found_keywords,
            "confidence": "medium" if len(all_keywords) > 5 else "low"
        }
    
    def _extract_with_tfidf(self, text: str) -> List[str]:
        """TF-IDFë¥¼ ì‚¬ìš©í•œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # ë‹¨ì–´ ì¶”ì¶œ
            words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
            text_for_tfidf = ' '.join(words)
            
            vectorizer = TfidfVectorizer(
                max_features=100,
                stop_words='english',
                ngram_range=(1, 2)
            )
            
            tfidf_matrix = vectorizer.fit_transform([text_for_tfidf])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
            keyword_scores = list(zip(feature_names, tfidf_scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [kw[0] for kw in keyword_scores[:10] if kw[1] > 0.1]
        
        except:
            return []
    
    def _flatten_keywords(self, categories: Dict) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°"""
        all_keywords = []
        for category_keywords in categories.values():
            if isinstance(category_keywords, list):
                all_keywords.extend(category_keywords)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        return list(set(all_keywords))[:20]  # ìµœëŒ€ 20ê°œ
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        words = re.findall(r'\b[a-zA-Z]{3,}\b', text)
        # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        word_counts = Counter(words)
        return [word for word, count in word_counts.most_common(15)]
    
    async def extract_keywords(self, pdf_path: str) -> Dict:
        """ë©”ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ - Gemini ìš°ì„ """
        print(f"ğŸ“„ PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        # 1. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        cleaned_text = self.clean_text(text)
        
        if len(cleaned_text) < 100:
            raise Exception("ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        print(f"ğŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(cleaned_text)} ê¸€ì")
        
        # 2. Gemini API ì‹œë„ (ìš°ì„ ìˆœìœ„)
        if self.is_gemini_configured():
            try:
                print("ğŸ¤– Gemini APIë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„...")
                result = await self.extract_with_gemini(cleaned_text)
                print(f"âœ… Gemini ì¶”ì¶œ ì„±ê³µ: {len(result['keywords'])}ê°œ í‚¤ì›Œë“œ")
                return result
            
            except Exception as e:
                print(f"âŒ Gemini API ì‹¤íŒ¨: {str(e)}")
                print("ğŸ”„ OpenAIë¡œ í´ë°± ì‹œë„...")
        
        # 3. OpenAI API ì‹œë„ (í´ë°±)
        if self.is_openai_configured():
            try:
                print("ğŸ¤– OpenAI APIë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„...")
                result = await self.extract_with_openai(cleaned_text)
                print(f"âœ… OpenAI ì¶”ì¶œ ì„±ê³µ: {len(result['keywords'])}ê°œ í‚¤ì›Œë“œ")
                return result
            
            except Exception as e:
                print(f"âŒ OpenAI API ì‹¤íŒ¨: {str(e)}")
                print("ğŸ”„ ìˆ˜ë™ ì¶”ì¶œ ëª¨ë“œë¡œ ì „í™˜...")
        
        # 4. ìˆ˜ë™ ì¶”ì¶œ ë°©ì‹ ì‹¤í–‰
        result = self.extract_with_fallback(cleaned_text)
        print(f"âœ… ìˆ˜ë™ ì¶”ì¶œ ì™„ë£Œ: {len(result['keywords'])}ê°œ í‚¤ì›Œë“œ")
        return result 