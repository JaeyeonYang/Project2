import json
import os
from typing import List, Dict, Any
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from pathlib import Path

class LabMatcher:
    def __init__(self):
        self.labs_data = []
        self.vectorizer = None
        self.lab_vectors = None
        self.load_labs_data()
    
    def load_labs_data(self):
        """ì—°êµ¬ì‹¤ ë°ì´í„° ë¡œë“œ"""
        try:
            # DB_addall.pyì—ì„œ ìƒì„±ëœ database íŽ˜ì´ì§€ë¥¼ íŒŒì‹±í•´ì„œ ì—°êµ¬ì‹¤ ë°ì´í„° ì¶”ì¶œ
            database_file = Path("../labfinder/src/app/database/page.tsx")
            
            if database_file.exists():
                self._parse_database_file(database_file)
            else:
                # ì§ì ‘ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì—ì„œ íŒŒì‹±
                self._parse_txt_files()
            
            print(f"ðŸ“š ì—°êµ¬ì‹¤ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.labs_data)}ê°œ ì—°êµ¬ì‹¤")
            
            if self.labs_data:
                self._prepare_vectors()
        
        except Exception as e:
            print(f"âŒ ì—°êµ¬ì‹¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°
            self._load_dummy_data()
    
    def _parse_database_file(self, file_path):
        """database/page.tsx íŒŒì¼ì—ì„œ ì—°êµ¬ì‹¤ ë°ì´í„° íŒŒì‹±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # JSON ë°ì´í„° ë¶€ë¶„ ì¶”ì¶œ
            start_marker = 'const labs: Lab[] = '
            end_marker = '];'
            
            start_idx = content.find(start_marker)
            if start_idx != -1:
                start_idx += len(start_marker)
                end_idx = content.find(end_marker, start_idx) + 1
                json_str = content[start_idx:end_idx]
                
                self.labs_data = json.loads(json_str)
                print(f"âœ… Database íŒŒì¼ì—ì„œ {len(self.labs_data)}ê°œ ì—°êµ¬ì‹¤ ë¡œë“œ")
        
        except Exception as e:
            print(f"Database íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            self._parse_txt_files()
    
    def _parse_txt_files(self):
        """ì§ì ‘ txt íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        print("ðŸ“ TXT íŒŒì¼ë“¤ì—ì„œ ì§ì ‘ íŒŒì‹±...")
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ txt íŒŒì¼ë“¤ ì°¾ê¸°
        parent_dir = Path("../")
        txt_files = list(parent_dir.glob("*.txt"))
        
        lab_id_counter = 1
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # "Professor:" ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
                entries = content.split("Professor:")[1:]
                
                major_name = txt_file.stem.replace("_", " ").title()
                
                for entry in entries:
                    lines = entry.strip().split('\n')
                    if not lines:
                        continue
                    
                    name = lines[0].strip()
                    keywords = ""
                    introduction = ""
                    current_section = None
                    
                    for line in lines[1:]:
                        line = line.strip()
                        if line == "Analysis:":
                            current_section = "analysis"
                            continue
                        elif line == "Introduction:":
                            current_section = "introduction"
                            continue
                        elif not line:
                            continue
                        
                        if current_section == "analysis":
                            keywords += line + ", "
                        elif current_section == "introduction":
                            introduction += line + " "
                    
                    lab = {
                        "id": f"lab-{lab_id_counter}",
                        "name": name,
                        "major": major_name,
                        "keywords": keywords.strip(", "),
                        "introduction": introduction.strip()
                    }
                    
                    self.labs_data.append(lab)
                    lab_id_counter += 1
            
            except Exception as e:
                print(f"íŒŒì¼ {txt_file} íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def _load_dummy_data(self):
        """ë”ë¯¸ ë°ì´í„° ë¡œë“œ (ë°±ì—…ìš©)"""
        self.labs_data = [
            {
                "id": "dummy-1",
                "name": "AI Research Lab",
                "major": "Computer Science",
                "keywords": "artificial intelligence, machine learning, deep learning, neural networks, computer vision",
                "introduction": "Leading research in artificial intelligence and machine learning applications."
            },
            {
                "id": "dummy-2", 
                "name": "Robotics Lab",
                "major": "Mechanical Engineering",
                "keywords": "robotics, autonomous systems, control systems, sensor fusion, navigation",
                "introduction": "Advanced robotics research focusing on autonomous navigation and manipulation."
            }
        ]
        print("ðŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™”")
    
    def _prepare_vectors(self):
        """ì—°êµ¬ì‹¤ í‚¤ì›Œë“œ ë²¡í„°í™”"""
        if not self.labs_data:
            return
        
        # ëª¨ë“  ì—°êµ¬ì‹¤ì˜ í‚¤ì›Œë“œ í…ìŠ¤íŠ¸
        lab_texts = []
        for lab in self.labs_data:
            text = f"{lab['keywords']} {lab['introduction']}"
            lab_texts.append(text)
        
        # TF-IDF ë²¡í„°í™”
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            ngram_range=(1, 2),
            lowercase=True
        )
        
        self.lab_vectors = self.vectorizer.fit_transform(lab_texts)
        print(f"âœ… {len(lab_texts)}ê°œ ì—°êµ¬ì‹¤ ë²¡í„°í™” ì™„ë£Œ")
    
    def calculate_similarity(self, cv_keywords: List[str]) -> List[Dict[str, Any]]:
        """CV í‚¤ì›Œë“œì™€ ì—°êµ¬ì‹¤ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not self.labs_data or not self.vectorizer:
            return []
        
        try:
            # CV í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
            cv_text = " ".join(cv_keywords)
            
            # CV í…ìŠ¤íŠ¸ ë²¡í„°í™”
            cv_vector = self.vectorizer.transform([cv_text])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(cv_vector, self.lab_vectors)[0]
            
            # ê²°ê³¼ ì •ë¦¬
            results = []
            for i, lab in enumerate(self.labs_data):
                similarity_score = float(similarities[i])
                
                # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°
                lab_keywords = self._extract_keywords(lab['keywords'])
                common_keywords = self._find_common_keywords(cv_keywords, lab_keywords)
                
                result = {
                    "id": lab["id"],
                    "name": lab["name"],
                    "major": lab["major"],
                    "keywords": lab["keywords"],
                    "introduction": lab["introduction"],
                    "similarity_score": similarity_score,
                    "common_keywords": common_keywords,
                    "match_count": len(common_keywords)
                }
                
                results.append(result)
            
            # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
            results.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            print(f"ðŸŽ¯ {len(results)}ê°œ ì—°êµ¬ì‹¤ ë§¤ì¹­ ì™„ë£Œ")
            return results
        
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _extract_keywords(self, keyword_text: str) -> List[str]:
        """í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not keyword_text:
            return []
        
        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
        keywords = [kw.strip().lower() for kw in keyword_text.split(',')]
        return [kw for kw in keywords if kw]
    
    def _find_common_keywords(self, cv_keywords: List[str], lab_keywords: List[str]) -> List[str]:
        """ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°"""
        cv_set = set([kw.lower().strip() for kw in cv_keywords])
        lab_set = set([kw.lower().strip() for kw in lab_keywords])
        
        # ì •í™•í•œ ë§¤ì¹˜
        exact_matches = cv_set.intersection(lab_set)
        
        # ë¶€ë¶„ ë§¤ì¹˜ (í¬í•¨ ê´€ê³„)
        partial_matches = set()
        for cv_kw in cv_set:
            for lab_kw in lab_set:
                if cv_kw in lab_kw or lab_kw in cv_kw:
                    if len(cv_kw) > 2 and len(lab_kw) > 2:  # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
                        partial_matches.add(max(cv_kw, lab_kw, key=len))
        
        return list(exact_matches.union(partial_matches))
    
    def get_top_recommendations(self, cv_keywords: List[str], top_n: int = 10) -> List[Dict[str, Any]]:
        """ìƒìœ„ Nê°œ ì¶”ì²œ ì—°êµ¬ì‹¤ ë°˜í™˜"""
        all_results = self.calculate_similarity(cv_keywords)
        
        # ìµœì†Œ ìœ ì‚¬ë„ ìž„ê³„ê°’ ì ìš© (ë„ˆë¬´ ë‚®ì€ ì ìˆ˜ëŠ” ì œì™¸)
        filtered_results = [
            result for result in all_results 
            if result["similarity_score"] > 0.01 or result["match_count"] > 0
        ]
        
        return filtered_results[:top_n]
    
    def get_lab_by_id(self, lab_id: str) -> Dict[str, Any]:
        """IDë¡œ íŠ¹ì • ì—°êµ¬ì‹¤ ì •ë³´ ì¡°íšŒ"""
        for lab in self.labs_data:
            if lab["id"] == lab_id:
                return lab
        return None 