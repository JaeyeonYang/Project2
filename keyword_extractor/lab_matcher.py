import json
import os
from typing import List, Dict, Any, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import re
from pathlib import Path
from difflib import SequenceMatcher
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
import nltk
from collections import Counter
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class LabMatcher:
    def __init__(self, data_path: str = None):
        self.labs_data = []
        self.vectorizer = None
        self.lab_vectors = None
        self.stop_words = set(stopwords.words('english'))
        self.data_path = data_path or os.path.join(os.path.dirname(__file__), '..', 'labfinder', 'src', 'app', 'database', 'page.tsx')
        self.load_labs_data()
    
    def load_labs_data(self):
        """ì—°êµ¬ì‹¤ ë°ì´í„° ë¡œë“œ"""
        try:
            database_file = Path(self.data_path)
            
            if database_file.exists():
                self._parse_database_file(database_file)
            else:
                logger.warning(f"Database file not found at {database_file}")
                self._parse_txt_files()
            
            logger.info(f"Loaded {len(self.labs_data)} labs")
            
            if self.labs_data:
                self._prepare_vectors()
            else:
                logger.warning("No lab data available, loading dummy data")
                self._load_dummy_data()
        
        except Exception as e:
            logger.error(f"Failed to load lab data: {str(e)}")
            self._load_dummy_data()
    
    def _parse_database_file(self, file_path):
        """database/page.tsx íŒŒì¼ì—ì„œ ì—°êµ¬ì‹¤ ë°ì´í„° íŒŒì‹±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # JSON ë°ì´í„° ë¶€ë¶„ ì¶”ì¶œ
            start_marker = 'const labs: Lab[] = '
            end_marker = '];'
            
            start_idx = content.find(start_marker)
            if start_idx != -1:
                start_idx += len(start_marker)
                end_idx = content.find(end_marker, start_idx) + 1
                json_str = content[start_idx:end_idx]
                
                # JSON íŒŒì‹± ì „ì— ë°ì´í„° ì •ë¦¬
                json_str = json_str.replace("'", '"')  # ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë³€ê²½
                
                try:
                    self.labs_data = json.loads(json_str)
                    logger.info(f"Successfully loaded {len(self.labs_data)} labs from database file")
                except json.JSONDecodeError as e:
                    logger.error(f"JSON parsing error: {str(e)}")
                    logger.error(f"Problematic JSON string: {json_str[:200]}...")  # ì²˜ìŒ 200ìë§Œ ë¡œê¹…
                    self._parse_txt_files()
            else:
                logger.warning("Could not find labs data in database file")
                self._parse_txt_files()
        
        except Exception as e:
            logger.error(f"Database file parsing failed: {str(e)}")
            self._parse_txt_files()
    
    def _parse_txt_files(self):
        """ì§ì ‘ txt íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        print("ğŸ“ TXT íŒŒì¼ë“¤ì—ì„œ ì§ì ‘ íŒŒì‹±...")
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ txt íŒŒì¼ë“¤ ì°¾ê¸°
        parent_dir = Path("../")
        txt_files = list(parent_dir.glob("*.txt"))
        
        lab_id_counter = 1
        
        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # "Professor:" ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
                entries = content.split("Professor:")[1:]
                
                major_name = txt_file.stem.replace("_", " ").title()
                
                for entry in entries:
                    lines = entry.strip().split('\n')
                    if not lines:
                        continue
                    
                    name = lines[0].strip()
                    keywords = ""
                    introduction = ""
                    current_section = None
                    
                    for line in lines[1:]:
                        line = line.strip()
                        if line == "Analysis:":
                            current_section = "analysis"
                            continue
                        elif line == "Introduction:":
                            current_section = "introduction"
                            continue
                        elif not line:
                            continue
                        
                        if current_section == "analysis":
                            keywords += line + ", "
                        elif current_section == "introduction":
                            introduction += line + " "
                    
                    lab = {
                        "id": f"lab-{lab_id_counter}",
                        "name": name,
                        "major": major_name,
                        "keywords": keywords.strip(", "),
                        "introduction": introduction.strip()
                    }
                    
                    self.labs_data.append(lab)
                    lab_id_counter += 1
            
            except Exception as e:
                print(f"íŒŒì¼ {txt_file} íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def _load_dummy_data(self):
        """ë”ë¯¸ ë°ì´í„° ë¡œë“œ (ë°±ì—…ìš©)"""
        self.labs_data = [
            {
                "id": "dummy-1",
                "name": "AI Research Lab",
                "major": "Computer Science",
                "keywords": "artificial intelligence, machine learning, deep learning, neural networks, computer vision",
                "introduction": "Leading research in artificial intelligence and machine learning applications."
            },
            {
                "id": "dummy-2", 
                "name": "Robotics Lab",
                "major": "Mechanical Engineering",
                "keywords": "robotics, autonomous systems, control systems, sensor fusion, navigation",
                "introduction": "Advanced robotics research focusing on autonomous navigation and manipulation."
            }
        ]
        print("ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ì´ˆê¸°í™”")
    
    def _prepare_vectors(self):
        """ì—°êµ¬ì‹¤ í‚¤ì›Œë“œ ë²¡í„°í™”"""
        if not self.labs_data:
            return
        
        # ì—°êµ¬ì‹¤ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
        lab_texts = []
        for lab in self.labs_data:
            # ê° ì„¹ì…˜ë³„ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥´ê²Œ ì ìš©
            sections = {
                'keywords': 2.0,  # í‚¤ì›Œë“œì— ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜
                'introduction': 1.5,  # ì†Œê°œë¬¸ì— ì¤‘ê°„ ê°€ì¤‘ì¹˜
                'major': 1.0,  # í•™ê³¼ì— ê¸°ë³¸ ê°€ì¤‘ì¹˜
                'university': 0.5  # ëŒ€í•™ì— ë‚®ì€ ê°€ì¤‘ì¹˜
            }
            
            # ê° ì„¹ì…˜ì„ ê°€ì¤‘ì¹˜ì— ë”°ë¼ ë°˜ë³µ
            text_parts = []
            for section, weight in sections.items():
                if section in lab:
                    # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì„¹ì…˜ì„ ë°˜ë³µ
                    text_parts.extend([lab[section]] * int(weight))
            
            lab_texts.append(' '.join(text_parts))
        
        # TF-IDF ë²¡í„°í™” íŒŒë¼ë¯¸í„° ìµœì í™”
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 3),  # 1-3 ë‹¨ì–´ ì¡°í•©ê¹Œì§€ ê³ ë ¤
            lowercase=True,
            min_df=1,
            max_df=0.9,
            sublinear_tf=True,  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§
            use_idf=True,
            smooth_idf=True,
            norm='l2'
        )
        
        self.lab_vectors = self.vectorizer.fit_transform(lab_texts)
        logger.info(f"Vectorized {len(lab_texts)} labs")
    
    def _find_common_keywords(self, cv_keywords: List[str], lab_keywords: List[str]) -> List[str]:
        """ê°œì„ ëœ ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸° (ë”ìš± ì™„í™”ëœ ë²„ì „)"""
        cv_set = set([kw.lower().strip() for kw in cv_keywords])
        lab_set = set([kw.lower().strip() for kw in lab_keywords])
        
        # ì •í™•í•œ ë§¤ì¹˜
        exact_matches = cv_set.intersection(lab_set)
        
        # ë¶€ë¶„ ë§¤ì¹˜ (ë”ìš± ì™„í™”ëœ ì•Œê³ ë¦¬ì¦˜)
        partial_matches = set()
        
        # 1. ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ (ì„ê³„ê°’ ë” ë‚®ì¶¤)
        for cv_kw in cv_set:
            for lab_kw in lab_set:
                # SequenceMatcherë¥¼ ì‚¬ìš©í•œ ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = SequenceMatcher(None, cv_kw, lab_kw).ratio()
                if similarity > 0.3:  # 50%ì—ì„œ 30%ë¡œ ë‚®ì¶¤
                    partial_matches.add(max(cv_kw, lab_kw, key=len))
                # í¬í•¨ ê´€ê³„ í™•ì¸ (ë”ìš± ìœ ì—°í•˜ê²Œ)
                elif any(token in lab_kw for token in cv_kw.split()) or any(token in cv_kw for token in lab_kw.split()):
                    partial_matches.add(max(cv_kw, lab_kw, key=len))
        
        # 2. í† í°í™” ê¸°ë°˜ ë§¤ì¹­ (ë”ìš± ìœ ì—°í•˜ê²Œ)
        cv_tokens = set()
        lab_tokens = set()
        
        for kw in cv_set:
            cv_tokens.update(word_tokenize(kw.lower()))
        for kw in lab_set:
            lab_tokens.update(word_tokenize(kw.lower()))
        
        # ë¶ˆìš©ì–´ ì œê±°
        cv_tokens = cv_tokens - self.stop_words
        lab_tokens = lab_tokens - self.stop_words
        
        # í† í° ê¸°ë°˜ ë§¤ì¹­ (ë”ìš± ìœ ì—°í•˜ê²Œ)
        token_matches = cv_tokens.intersection(lab_tokens)
        for token in token_matches:
            # í† í°ì´ í¬í•¨ëœ ì›ë˜ í‚¤ì›Œë“œ ì°¾ê¸°
            for cv_kw in cv_set:
                if token in cv_kw:
                    partial_matches.add(cv_kw)
            for lab_kw in lab_set:
                if token in lab_kw:
                    partial_matches.add(lab_kw)
        
        return list(exact_matches.union(partial_matches))
    
    def _calculate_similarity_scores(self, cv_keywords: List[str], lab: Dict[str, Any]) -> Tuple[float, float, float, float]:
        """ê° ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ë”ìš± ì™„í™”ëœ ë²„ì „)"""
        # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        lab_keywords = self._extract_keywords(lab['keywords'])
        common_keywords = self._find_common_keywords(cv_keywords, lab_keywords)
        keyword_score = len(common_keywords) / max(len(cv_keywords), len(lab_keywords))
        
        # 2. ì—°êµ¬ ë¶„ì•¼ ë§¤ì¹­ ì ìˆ˜ (ë”ìš± ì™„í™”)
        research_score = 0.0
        for cv_kw in cv_keywords:
            for lab_kw in lab_keywords:
                similarity = SequenceMatcher(None, cv_kw.lower(), lab_kw.lower()).ratio()
                if similarity > 0.3:  # 0.4ì—ì„œ 0.3ìœ¼ë¡œ ë‚®ì¶¤
                    research_score += 1.0
                # ë¶€ë¶„ ì¼ì¹˜ë„ ê³ ë ¤ (ë” ë§ì€ ì ìˆ˜)
                elif any(token in lab_kw.lower() for token in cv_kw.lower().split()):
                    research_score += 0.7
        research_score = min(research_score / len(cv_keywords), 1.0)
        
        # 3. í•™ê³¼ ë§¤ì¹­ ì ìˆ˜ (ë”ìš± ì™„í™”)
        major_score = 0.0
        for kw in cv_keywords:
            if kw.lower() in lab['major'].lower():
                major_score = 1.0
                break
            # ë¶€ë¶„ ì¼ì¹˜ë„ ê³ ë ¤ (ë” ë§ì€ ì ìˆ˜)
            elif any(token in lab['major'].lower() for token in kw.lower().split()):
                major_score = 0.7
                break
        
        # 4. ëŒ€í•™ ë§¤ì¹­ ì ìˆ˜ (ë”ìš± ì™„í™”)
        university_score = 0.0
        for kw in cv_keywords:
            if kw.lower() in lab['university'].lower():
                university_score = 0.7
                break
            # ë¶€ë¶„ ì¼ì¹˜ë„ ê³ ë ¤ (ë” ë§ì€ ì ìˆ˜)
            elif any(token in lab['university'].lower() for token in kw.lower().split()):
                university_score = 0.4
                break
        
        return keyword_score, research_score, major_score, university_score
    
    def calculate_similarity(self, cv_keywords: List[str]) -> List[Dict[str, Any]]:
        """CV í‚¤ì›Œë“œì™€ ì—°êµ¬ì‹¤ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not self.labs_data or not self.vectorizer:
            logger.warning("No lab data or vectorizer available")
            return []
        
        try:
            # CV í‚¤ì›Œë“œë¥¼ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ê²°í•©
            cv_text = ' '.join(cv_keywords)
            cv_vector = self.vectorizer.transform([cv_text])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = cosine_similarity(cv_vector, self.lab_vectors)[0]
            
            results = []
            for i, lab in enumerate(self.labs_data):
                # ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜
                base_similarity = float(similarities[i])
                
                # ì„¸ë¶€ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
                keyword_score, research_score, major_score, university_score = self._calculate_similarity_scores(cv_keywords, lab)
                
                # ìµœì¢… ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ì¹˜ ì¡°ì •)
                final_score = (
                    base_similarity * 0.3 +  # ê¸°ë³¸ ìœ ì‚¬ë„
                    keyword_score * 0.3 +  # í‚¤ì›Œë“œ ë§¤ì¹­
                    research_score * 0.2 +  # ì—°êµ¬ ë¶„ì•¼ ë§¤ì¹­
                    major_score * 0.15 +  # í•™ê³¼ ë§¤ì¹­
                    university_score * 0.05  # ëŒ€í•™ ë§¤ì¹­
                )
                
                # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê¹…
                logger.debug(f"Lab: {lab['name']}, Final Score: {final_score:.3f}, Base: {base_similarity:.3f}, "
                           f"Keyword: {keyword_score:.3f}, Research: {research_score:.3f}")
                
                result = {
                    "id": lab["id"],
                    "name": lab["name"],
                    "major": lab["major"],
                    "university": lab["university"],
                    "keywords": lab["keywords"],
                    "introduction": lab["introduction"],
                    "similarity_score": final_score,
                    "base_similarity": base_similarity,
                    "keyword_score": keyword_score,
                    "research_score": research_score,
                    "major_score": major_score,
                    "university_score": university_score,
                    "common_keywords": self._find_common_keywords(cv_keywords, self._extract_keywords(lab['keywords'])),
                    "match_count": len(self._find_common_keywords(cv_keywords, self._extract_keywords(lab['keywords'])))
                }
                
                results.append(result)
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
            results.sort(key=lambda x: x["similarity_score"], reverse=True)
            logger.info(f"Calculated similarity for {len(results)} labs")
            return results
        
        except Exception as e:
            logger.error(f"Failed to calculate similarity: {str(e)}")
            return []
    
    def _extract_keywords(self, keyword_text: str) -> List[str]:
        """í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ì—ì„œ ê°œë³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not keyword_text:
            return []
        
        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì •ë¦¬
        keywords = [kw.strip().lower() for kw in keyword_text.split(',')]
        return [kw for kw in keywords if kw]
    
    def get_top_recommendations(self, cv_keywords: List[str], top_n: int = 10) -> List[Dict[str, Any]]:
        """ìƒìœ„ Nê°œ ì¶”ì²œ ì—°êµ¬ì‹¤ ë°˜í™˜ (ë”ìš± ì™„í™”ëœ ë²„ì „)"""
        all_results = self.calculate_similarity(cv_keywords)
        
        if not all_results:
            logger.warning("No results found in similarity calculation")
            return []
        
        # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš© (ë”ìš± ë‚®ì¶¤)
        filtered_results = [
            result for result in all_results 
            if result["similarity_score"] > 0.0001 or result["match_count"] > 0  # 0.001ì—ì„œ 0.0001ë¡œ ë‚®ì¶¤
        ]
        
        logger.info(f"Filtered results: {len(filtered_results)} labs after threshold")
        
        if len(filtered_results) <= top_n:
            return filtered_results
        
        # ëŒ€í•™ë³„ ê·¸ë£¹í™” ë° ì„ íƒ
        university_groups = {}
        for result in filtered_results:
            university = result["university"]
            if university not in university_groups:
                university_groups[university] = []
            university_groups[university].append(result)
        
        # ê° ëŒ€í•™ì—ì„œ ìµœëŒ€ 4ê°œì”© ì„ íƒ (3ê°œì—ì„œ 4ê°œë¡œ ì¦ê°€)
        selected_results = []
        remaining_slots = top_n
        
        # 1. ê° ëŒ€í•™ì—ì„œ ìƒìœ„ ê²°ê³¼ ì„ íƒ
        for university, results in university_groups.items():
            if remaining_slots > 0:
                selected_count = min(4, len(results), remaining_slots)  # 3ì—ì„œ 4ë¡œ ì¦ê°€
                selected_results.extend(results[:selected_count])
                remaining_slots -= selected_count
        
        # 2. ë‚¨ì€ ìë¦¬ ì±„ìš°ê¸°
        if remaining_slots > 0:
            remaining_results = [
                result for result in filtered_results 
                if result not in selected_results
            ]
            selected_results.extend(remaining_results[:remaining_slots])
        
        # ìµœì¢… ê²°ê³¼ ì •ë ¬
        selected_results.sort(key=lambda x: x["similarity_score"], reverse=True)
        logger.info(f"Final recommendations: {len(selected_results)} labs")
        return selected_results
    
    def get_lab_by_id(self, lab_id: str) -> Dict[str, Any]:
        """IDë¡œ íŠ¹ì • ì—°êµ¬ì‹¤ ì •ë³´ ì¡°íšŒ"""
        for lab in self.labs_data:
            if lab["id"] == lab_id:
                return lab
        return None 